\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{amssymb,amsmath,listings}

\title{A Randomized Algorithm for $\alpha$-approximate Sortedness}
\date{9 May 2016}
\author{Brian Sheedy, Avi Weinstock}

\begin{document}
\maketitle{}
\section{Introduction}
\paragraph{}Deterministically evaluating whether a list of elements is sorted or not is trivial to do in linear time by comparing each element to the next.
However, when run on extremely large lists, such as those of cardinality greater than $10^9$, this linear time complexity can become prohibitive.
Thus, we propose a randomized algorithm for determining whether a list of size $n$ is $\alpha$-approximately sorted, meaning that at least $\lceil\alpha n\rceil$ elements are sorted, in sub-linear time.
We provide both a mathematical analysis of the algorithm as well as empirical testing on an array of size $10^9$.
\section{Algorithm}
\paragraph{}For $\beta$ uniformly random sub-array samplings of length $\gamma$, we have the following algorithm for determining whether the provided list $A$ is $\alpha$-approximately sorted or not:
\begin{lstlisting}[escapeinside={(*}{*)}]
total = 0
for i = 1, 2, ... (*$\beta$*):
    r (*$\in_{R}$*) {1, 2, ..., n - (*$\gamma$*)}
    (*$s_{r}$*) = sub array starting at (*$r^{th}$*) element in (*$A$*)
    (*$\alpha_{r}$*) = (*$\alpha$*)-sortedness of (*$s_{r}$*)
    total += (*$\alpha_{r}$*)
return (total / (*$\beta$*)) (*$\geq$*) (*$\alpha$*)
\end{lstlisting}
\paragraph{}What this achieves is randomly pick $\beta$ sub-arrays of length $\gamma$, calculate each sub-array's $\alpha$-sortedness, and then average over all $\beta$ sub-arrays.
 If the average $\alpha$-sortedness is greater than or equal to the specified $\alpha$-sortedness, then we guess that the entire array is $\alpha$-approximately sorted.
The idea is that by randomly sampling a number of smaller arrays and averaging their $\alpha$-sortedness, we can get a good representation of the entire array's $\alpha$-sortedness without going through the entire array.
 This is similar to how a large list of numbers can have its sum estimated by randomly sampling a small number and scaling the result.
\paragraph{}While it is possible to store arrays of cardinality $\geq 10^{9}$ in memory, it is impractical to do so.
 This algorithm only requires random access to elements of the array, then stores a sub-array in memory, so it only takes up $O(\gamma)$ space.
This could be achieved by means such as making queries to elements stored in a database or reading at random offsets in a file containing the elements.
In terms of runtime complexity, it requires $\beta$ repetitions of choosing an array and calculating its $\alpha$-sortedness.
 We assume the array is not stored in memory, thus requiring the sub-array to be copied. Thus, the sub-array choosing takes $O(\gamma)$ time to copy each element of the sub-array.
The $\alpha$-sortedness can be done in linear time and constant space by checking if the current element is greater than the next element.
 If so, then the element is out of order.
 By keeping track of how many elements are out of order, then dividing the result by $\gamma$, the $\alpha$-sortedness can be calculated easily.
Thus, the total space complexity is $O(\gamma)$ while the total time complexity is $O(\beta \cdot \gamma)$
\section{Theoretical Analysis}
\paragraph{}Define $r$ as a uniformly randomly chosen element of the set $\{1, 2, ..., n - \gamma\}$ and $s_{i}$ as the $\gamma$ length sub-array starting at the $i^{th}$ element of $A$.
 Then, $s_{r}$ is a sub-array of length $\gamma$ chosen uniformly at random.
 Also define $\alpha_{i}$ as the $\alpha$-sortedness of $s_{i}$.
Since $\alpha$-sortedness is the fraction of elements in the array that are sorted, $\alpha_{i}$ can be represented as $\frac{1}{\gamma} \cdot t_{i}$, where $t_{i}$ is the number of elements in $s_{i}$ that are sorted.
 Since each element is either sorted or not, $t_{i} = \sum_{j=i}^{i+\gamma}X_{j}$ where:
\[ 
X_{j} =
\begin{cases}
	1 & j^{th}\text{ element of A is sorted} \\
	0 & \text{otherwise}
\end{cases} \]
\paragraph{}Since $r$ is chosen uniformly at random, we know that in expectation $\alpha_{r}$ is the average of all $\alpha_{i}$ for $i = 1, 2, ..., n - \gamma$. Thus:
\begin{align*}
\mathbb E(\alpha_{r}) &= \frac{1}{n - \gamma} \cdot \sum_{i = 1}^{n - \gamma}\alpha_{i} \\
&= \frac{1}{n - \gamma} \cdot \sum_{i = 1}^{n - \gamma} \frac{1}{\gamma} \cdot \sum_{j = i}^{j + \gamma}X_{j} \\
%&= \frac{1}{\gamma \cdot (n - \gamma)} \cdot \left[ \gamma \cdot \left( \sum_{i=\gamma}^{n - \gamma + 1} X_{i} \right) + (X_{1} + X_{n}) + ... + (\gamma - 1) \cdot (X_{\gamma - 1} + X_{n - \gamma + 2})\right] \\
&= \frac{1}{\gamma \cdot (n - \gamma)} \cdot \left[ \gamma \cdot \left( \sum_{i=\gamma}^{n - \gamma + 1} X_{i} \right) + \sum_{i=1}^{\gamma-1}i\cdot\left(X_{i} + X_{n-(i-1)}\right)\right]
\end{align*}
\paragraph{}This last result comes from the observation that the elements in the middle of the array will be summed $\gamma$ times since $\gamma$ sub-arrays include $X_{j}$ for $\gamma \leq j \leq n - \gamma + 1$.
The elements at either end of the array, however, will be added fewer times, as the first and last elements are only in one sub-array, second and second to last are only in two sub-arrays, and so on.
Since these elements are being summed fewer times, we can upper-bound their coefficients by $\gamma$ to conclude (letting $\alpha'$ be the $\alpha$-sortedness of the entire array):
\begin{align*}
\mathbb E(\alpha_{r}) & \leq \frac{1}{\gamma \cdot (n - \gamma)} \cdot \gamma \cdot \sum_{j = 1}^{n} X_{j} \\
& \leq \frac{1}{n - \gamma} \cdot \sum_{j = 1}^{n} X_{j} \\
& \leq \frac{1}{n - \gamma} \cdot (n \cdot \alpha') \\
\end{align*}
If $\gamma$ is set to something that satisfies $(\underset{n\rightarrow\infty}{\text{lim}}\frac{\gamma}{n} = 0)$ (e.g. $(\gamma = \text{log}(n))$ or $(\gamma = \sqrt n)$) we can show that this is equal to $\alpha'$ in the limit:
\begin{align*}
\underset{n\rightarrow\infty}{\text{lim}}\mathbb E(\alpha_{r})
& \leq \underset{n\rightarrow\infty}{\text{lim}}\frac{n \cdot \alpha'}{n - \gamma} \\
& \leq \underset{n\rightarrow\infty}{\text{lim}}\frac{n \cdot \alpha'}{n(1-\frac{\gamma}{n})} \\
& \leq \underset{n\rightarrow\infty}{\text{lim}}\frac{\alpha'}{(1-\frac{\gamma}{n})} \\
& \leq \frac{\alpha'}{(1-0)} = \alpha'
\end{align*}

Thus, in expectation, the limit of a sub-array's $\alpha$-sortedness is upper-bounded by the $\alpha$-sortedness of the entire array.
In the case that the elements that are summed fewer times are all unsorted, the difference between the exact expected value of $\alpha_{r}$ and the upper bound is zero.
The opposite also applies, where all edge elements being sorted results in the exact value and upper bound being furthest apart.
 Let us show that even in the case where the two are furthest apart, the upper bound is still close to the exact value:
%\begin{equation*}
%\gamma \cdot (X_{1} + X_{n}) + ... + \gamma \cdot (X_{\gamma - 1} + X_{n - \gamma + 2}) = \gamma \cdot \left[ \sum_{j = 1}^{\gamma - 1} X_{j} + \sum_{n - \gamma + 2}^{n} X_{j}  \right]
%\end{equation*}
\begin{align*}
\gamma \cdot \left[ \sum_{j = 1}^{\gamma - 1} X_{j} + \sum_{n - \gamma + 2}^{n} X_{j}  \right] - \sum_{i=1}^{\gamma-1}i\cdot\left(X_{i} + X_{n-(i-1)}\right) &= \sum_{i=1}^{\gamma-1}\left(\gamma-i\right)\cdot\left(X_{i} + X_{n-(i-1)}\right) \\
%= (\gamma - 1) \cdot (X_{1} + X_{n}) + ... + (X_{\gamma - 1} + X_{n - \gamma + 2})
&= \sum_{i=1}^{\gamma-1}\left(\gamma-i\right)\cdot 2 \\
&= 2 \cdot \sum_{i=1}^{\gamma-1}\left(\gamma-i\right) \\
&= 2 \cdot \frac{\gamma \cdot (\gamma - 1)}{2} \\
&= \gamma \cdot (\gamma - 1)
\end{align*}
%\begin{equation*}
%(\gamma - 1) \cdot (X_{1} + X_{n}) + ... + (X_{\gamma - 1} + X_{n - \gamma + 2}) = \frac{\gamma \cdot (\gamma - 1)}{2}
%\end{equation*}
\paragraph{}This gives us the difference between the exact value and upper bound for the sums of the 0-1 valued random variables.
 We then turn this difference in 0-1 valued random variable sums into a difference in $\alpha$ values to get:
\begin{align*}
\alpha_{diff} &= \frac{1}{\gamma \cdot (n - \gamma)} \cdot \gamma \cdot (\gamma - 1) \\
&= \frac{\gamma - 1}{(n - \gamma)}
\end{align*}
\begin{equation*}
\alpha' - \frac{\gamma - 1}{(n - \gamma)} \leq \mathbb E(\alpha_{r}) \leq \alpha'
\end{equation*}
\paragraph{}In the case of a large $n$, which this algorithm is being evaluated under, and by setting $\gamma$ to a small value such as $\text{log}(n)$, the difference is close to $\frac{1}{n}$, which is small.
 Thus, $\mathbb E(\alpha_{r})$ is close to $\alpha'$.

% TODO: APPLY CHERNOFF OR SOMETHING LIKE THAT
Markov's inequality states that for any non-negative random variable $X$:
\begin{equation*}
\forall t>0(\text{Pr}(X \ge t) \le \frac{\mathbb E(X)}{t})
\end{equation*}
Since $\alpha_r$ is a 0-1 valued random variable, it is also a nonnegative random variable, so Markov's inequality applies to it.
This could be used used to bound the probability that it overestimates $\alpha'$:
\begin{equation*}
\text{Pr}(\alpha_r \ge \alpha')
\le \frac{\mathbb E(\alpha_r)}{\alpha'}
\le \frac{\alpha'}{\alpha'} = 1
\end{equation*}
Unfortunately, this provides us with no new information.

%Chebyshev's inequality would require the variance, which we have not yet analyzed.
Chernoff-style bounds cannot be used in this context, as the $X_i$ random variables defined above are not independent unless the input is specifically chosen to make them so (which will not apply to real-world data, or data with any common (e.g. uniform, gaussian) distribution).

All the analysis above implicitly assumes $\beta=1$.
Taking $\beta$ random subarrays and averaging their approximations of $\alpha$ to produce $\hat{\alpha} = \sum_{i=1}^\beta\alpha_r^i$ would produce closer estimations of $\alpha'$ if $\mathbb E(\alpha_r)$ were exactly $\alpha'$ (i.e. if $\alpha_r$ were an unbiased estimator of $\alpha'$).
The bounds we've shown indicate that it undershoots in expectation by at most $\frac{\gamma - 1}{(n - \gamma)}$, which indicates that increasing $\beta$ may result in $\hat{\alpha}$ undershooting $\alpha'$ more consistantly.
This can be mitigated somewhat by decreasing the target $\alpha$ for the final comparison by some fraction of $\frac{\gamma - 1}{(n - \gamma)}$.
\section{Empirical Analysis}
\paragraph{}The code available in the appendix was run 1000 times for each combination of $\alpha, \alpha', \beta,$ and $\gamma$ on a list of cardinality $10^9$.
 The full results can be seen in the appendix in Table 1 through Table 9.
\paragraph{}In general, the algorithm performs as expected from the mathematical analysis. 
Increasing $\beta$ and $\gamma$ both improve the probability of the algorithm returning true when it should, albeit in different ways.
Increasing $\beta$ causes the algorithm to return true with higher probability, both when it should and when it should not.
Thus, increasing $\beta$ by itself will increase the probability that the algorithm returns true, but introduces more false-positives, where we are returned true when the array is not actually $\alpha$-approximately sorted.
\paragraph{}Increasing $\gamma$ reduces the number of false-positives while not significantly affecting the success probability when the array is $\alpha$-approximately sorted, with one caveat.
As pointed out in the mathematical analysis, the expected value of a random sub-array's $\alpha$ approaches $\alpha'$ as the size of the total array approaches infinity.
However, with a cardinality of $10^9$, the expected sub-array $\alpha$ is still slightly below $\alpha'$. 
This results in the algorithm returning true with very low or zero probability when run with $\alpha = \alpha'$ and with reasonable values such as $\beta = 1000$ and $\gamma = 1000$.
It will, however, return true with high probability for $\alpha$ slightly less than $\alpha'$, such as $\alpha = \alpha' - 0.01$ or $\alpha = \alpha' - 0.005$.
Thus, when run on lists that are on the smaller side of the problem space, the $\alpha$ given to the algorithm should be slightly lower than the actual desired $\alpha$ in order to achieve the best results.
This caveat of course does not apply to $\alpha = \alpha' = 1$, as the algorithm always returns true in that case.
\newpage
\section{Appendix}
\subsection{Empirical Results}
\paragraph{}Higher numbers in the top portion of tables (row of $\alpha = \alpha'$ and above) indicate higher success rate, while higher numbers below that row indicate higher failure rates.
\begin{center}
Table 1: Probability Algorithm Returned True With $\beta=10$, $\gamma=10$ \\
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\alpha$ & $\alpha' = 0.5$ & $\alpha' = 0.6$ & $\alpha' = 0.7$ & $\alpha' = 0.8$ & $\alpha' = 0.9$ & $\alpha' = 1.0$ \\
\hline
$\alpha' -0.01$ & 0.874 & 0.850 & 0.781 & 0.767 & 0.798 & 1.0 \\
\hline
$\alpha'-0.005$ & 0.836 & 0.787 & 0.750 & 0.725 & 0.684 & 1.0 \\
\hline
$\alpha'-0.001$ & 0.826 & 0.768 & 0.725 & 0.704 & 0.658 & 1.0 \\
\hline
$\alpha'$            & 0.836 & 0.759 & 0.715 & 0.682 & 0.647 & 1.0 \\
\hline
$\alpha'+0.001$ & 0.747 & 0.737 & 0.660 & 0.582 & 0.567 & N/A \\
\hline
$\alpha'+0.005$ & 0.767 & 0.699 & 0.678 & 0.596 & 0.533 & N/A \\
\hline
$\alpha'+0.01 $ & 0.780 & 0.698 & 0.639 & 0.605 & 0.521 & N/A \\
\hline
\end{tabular}
\end{center}

\begin{center}
Table 2: Probability Algorithm Returned True With $\beta=10$, $\gamma=100$ \\
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\alpha$ & $\alpha' = 0.5$ & $\alpha' = 0.6$ & $\alpha' = 0.7$ & $\alpha' = 0.8$ & $\alpha' = 0.9$ & $\alpha' = 1.0$ \\
\hline
$\alpha' -0.01$ & 0.657 & 0.637 & 0.647 & 0.696 & 0.812 & 1.0 \\
\hline
$\alpha'-0.005$ & 0.537 & 0.515 & 0.556 & 0.575 & 0.654 & 1.0 \\
\hline
$\alpha'-0.001$ & 0.470 & 0.465 & 0.473 & 0.501 & 0.506 & 1.0 \\
\hline
$\alpha'$         & 0.437 & 0.452 & 0.465 & 0.444 & 0.466 & 1.0 \\
\hline
$\alpha'+0.001$ & 0.432 & 0.455 & 0.426 & 0.416 & 0.434 & N/A \\
\hline
$\alpha'+0.005$ & 0.356 & 0.361 & 0.389 & 0.314 & 0.277 & N/A \\
\hline
$\alpha'+0.01 $ & 0.330 & 0.293 & 0.248 & 0.222 & 0.159 & N/A \\
\hline
\end{tabular}
\end{center}

\begin{center}
Table 3: Probability Algorithm Returned True With $\beta=10$, $\gamma=1000$ \\
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\alpha$ & $\alpha' = 0.5$ & $\alpha' = 0.6$ & $\alpha' = 0.7$ & $\alpha' = 0.8$ & $\alpha' = 0.9$ & $\alpha' = 1.0$ \\
\hline
$\alpha' -0.01$ & 0.574 & 0.628 & 0.620 & 0.722 & 0.871 & 1.0 \\
\hline
$\alpha'-0.005$ & 0.425 & 0.492 & 0.482 & 0.522 & 0.672 & 1.0 \\
\hline
$\alpha'-0.001$ & 0.415 & 0.403 & 0.409 & 0.385 & 0.441 & 1.0 \\
\hline
$\alpha'$            & 0.357 & 0.369 & 0.363 & 0.379 & 0.389 & 1.0 \\
\hline
$\alpha'+0.001$ & 0.356 & 0.345 & 0.314 & 0.332 & 0.307 & N/A \\
\hline
$\alpha'+0.005$ & 0.276 & 0.271 & 0.233 & 0.176 & 0.122 & N/A \\
\hline
$\alpha'+0.01 $ & 0.179 & 0.155 & 0.127 & 0.094 & 0.013 & N/A \\
\hline
\end{tabular}
\end{center}

\begin{center}
Table 4: Probability Algorithm Returned True With $\beta=100$, $\gamma=10$ \\
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\alpha$ & $\alpha' = 0.5$ & $\alpha' = 0.6$ & $\alpha' = 0.7$ & $\alpha' = 0.8$ & $\alpha' = 0.9$ & $\alpha' = 1.0$ \\
\hline
$\alpha' -0.01$ & 1.0 & 0.997 & 0.991 & 0.976 & 0.971 & 1.0 \\
\hline
$\alpha'-0.005$ & 0.999 & 0.990 & 0.978 & 0.940 & 0.926 & 1.0 \\
\hline
$\alpha'-0.001$ & 0.998 & 0.990 & 0.963 & 0.928 & 0.829 & 1.0 \\
\hline
$\alpha'$            & 0.992 & 0.991 & 0.950 & 0.880 & 0.799 & 1.0 \\
\hline
$\alpha'+0.001$ & 0.999 & 0.977 & 0.937 & 0.909 & 0.770 & N/A \\
\hline
$\alpha'+0.005$ & 0.0.990 & 0.964 & 0.906 & 0.808 & 0.628 & N/A \\
\hline
$\alpha'+0.01 $ & 0.980 & 0.932 & 0.831 & 0.687 & 0.409 & N/A \\
\hline
\end{tabular}
\end{center}

\newpage
\begin{center}
Table 5: Probability Algorithm Returned True With $\beta=100$, $\gamma=100$ \\
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\alpha$ & $\alpha' = 0.5$ & $\alpha' = 0.6$ & $\alpha' = 0.7$ & $\alpha' = 0.8$ & $\alpha' = 0.9$ & $\alpha' = 1.0$ \\
\hline
$\alpha' -0.01$ & 0.794 & 0.811 & 0.856 & 0.925 & 0.989 & 1.0 \\
\hline
$\alpha'-0.005$ & 0.584 & 0.591 & 0.619 & 0.683 & 0.842 & 1.0 \\
\hline
$\alpha'-0.001$ & 0.413 & 0.372 & 0.387 & 0.350 & 0.461 & 1.0 \\
\hline
$\alpha'$            & 0.350 & 0.321 & 0.275 & 0.316 & 0.344 & 1.0 \\
\hline
$\alpha'+0.001$ & 0.256 & 0.252 & 0.262 & 0.248 & 0.225 & N/A \\
\hline
$\alpha'+0.005$ & 0.134 & 0.099 & 0.094 & 0.067 & 0.029 & N/A \\
\hline
$\alpha'+0.01 $ & 0.041 & 0.032 & 0.010 & 0.010 & 0.0 & N/A \\
\hline
\end{tabular}
\end{center}

\begin{center}
Table 6: Probability Algorithm Returned True With $\beta=100$, $\gamma=1000$ \\
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\alpha$ & $\alpha' = 0.5$ & $\alpha' = 0.6$ & $\alpha' = 0.7$ & $\alpha' = 0.8$ & $\alpha' = 0.9$ & $\alpha' = 1.0$ \\
\hline
$\alpha' -0.01$ & 0.654 & 0.672 & 0.769 & 0.940 & 1.0 & 1.0 \\
\hline
$\alpha'-0.005$ & 0.322 & 0.337 & 0.426 & 0.571 & 0.904 & 1.0 \\
\hline
$\alpha'-0.001$ & 0.123 & 0.142 & 0.150 & 0.173 & 0.245 & 1.0 \\
\hline
$\alpha'$            & 0.110 & 0.093 & 0.092 & 0.099 & 0.091 & 1.0 \\
\hline
$\alpha'+0.001$ & 0.058 & 0.060 & 0.058 & 0.053 & 0.041 & N/A \\
\hline
$\alpha'+0.005$ & 0.011 & 0.010 & 0.007 & 0.001 & 0.0 & N/A \\
\hline
$\alpha'+0.01 $ & 0.002 & 0.0 & 0.0 & 0.0 & 0.0 & N/A \\
\hline
\end{tabular}
\end{center}

\begin{center}
Table 7: Probability Algorithm Returned True With $\beta=1000$, $\gamma=10$ \\
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\alpha$ & $\alpha' = 0.5$ & $\alpha' = 0.6$ & $\alpha' = 0.7$ & $\alpha' = 0.8$ & $\alpha' = 0.9$ & $\alpha' = 1.0$ \\
\hline
$\alpha' -0.01$ & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
\hline
$\alpha'-0.005$ & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
\hline
$\alpha'-0.001$ & 1.0 & 1.0 & 1.0 & 1.0 & 0.999 & 1.0 \\
\hline
$\alpha'$            & 1.0 & 1.0 & 1.0 & 1.0 & 0.995 & 1.0 \\
\hline
$\alpha'+0.001$ & 1.0 & 1.0 & 1.0 & 1.0 & 0.991 & N/A \\
\hline
$\alpha'+0.005$ & 1.0 & 1.0 & 1.0 & 0.998 & 0.830 & N/A \\
\hline
$\alpha'+0.01 $ & 1.0 & 1.0 & 0.997 & 0.933 & 0.235 & N/A \\
\hline
\end{tabular}
\end{center}

\begin{center}
Table 8: Probability Algorithm Returned True With $\beta=1000$, $\gamma=100$ \\
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\alpha$ & $\alpha' = 0.5$ & $\alpha' = 0.6$ & $\alpha' = 0.7$ & $\alpha' = 0.8$ & $\alpha' = 0.9$ & $\alpha' = 1.0$ \\
\hline
$\alpha' -0.01$ & 0.995 & 0.995 & 1.0 & 1.0 & 1.0 & 1.0 \\
\hline
$\alpha'-0.005$ & 0.762 & 0.723 & 0.795 & 0.936 & 1.0 & 1.0 \\
\hline
$\alpha'-0.001$ & 0.166 & 0.136 & 0.117 & 0.136 & 0.326 & 1.0 \\
\hline
$\alpha'$            & 0.069 & 0.048 & 0.039 & 0.060 & 0.075 & 1.0 \\
\hline
$\alpha'+0.001$ & 0.030 & 0.019 & 0.012 & 0.008 & 0.012 & N/A \\
\hline
$\alpha'+0.005$ & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & N/A \\
\hline
$\alpha'+0.01 $ & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & N/A \\
\hline
\end{tabular}
\end{center}

\begin{center}
Table 9: Probability Algorithm Returned True With $\beta=1000$, $\gamma=1000$ \\
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\alpha$ & $\alpha' = 0.5$ & $\alpha' = 0.6$ & $\alpha' = 0.7$ & $\alpha' = 0.8$ & $\alpha' = 0.9$ & $\alpha' = 1.0$ \\
\hline
$\alpha' -0.01$ & 0.843 & 0.934 & 0.996 & 1.0 & 1.0 & 1.0 \\
\hline
$\alpha'-0.005$ & 0.053 & 0.097 & 0.231 & 0.699 & 1.0 & 1.0 \\
\hline
$\alpha'-0.001$ & 0.0 & 0.0 & 0.0 & 0.0 & 0.011 & 1.0 \\
\hline
$\alpha'$            & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 1.0 \\
\hline
$\alpha'+0.001$ & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & N/A \\
\hline
$\alpha'+0.005$ & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & N/A \\
\hline
$\alpha'+0.01 $ & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & N/A \\
\hline
\end{tabular}
\end{center}

\newpage
\subsection{C Code}
\begin{lstlisting}
#include <stdio.h>
#include <stdlib.h>

int* genAlphaSortedArray(float alpha, int n){
    int i;
    int* array = calloc(n, sizeof(int));
    for(i = 0; i < n; i++){
        array[i] = i;
    }
    int num_replace = (int)((1 - alpha) * n);
    for(i = 0; i < num_replace; i++){
        int index = (int)(rand() % n);
        if(array[index] == n + 1){
            i--;
            continue;
        }
        array[index] = n + 1;
    }
    return array;
}

int* getRandomSubArrayLocal(int* array, int gamma, int n){
    int index = (int)(rand() % (n - gamma));
    return &array[index];
}

float calcAlphaSortedness(int* sub_array, int gamma, int n){
    (void)n;
    int i;
    int num_unsorted = 0;
    for(i = 0; i < gamma - 1; i++){
        if(sub_array[i] >= sub_array[i + 1]){
            num_unsorted++;
        }
    }
    return (1.0 * (gamma - num_unsorted)) / gamma;
}

int main(int argc, char* argv[]){
    time_t t;
    int i, rep;
    srand((unsigned int) time(&t));

    if(argc != 7){
        printf("Usage is %s array_size alpha beta gamma 
                array_alpha num_repetitions\n", argv[0]);
        exit(EXIT_FAILURE);
    }

    int n = atoi(argv[1]);
    float alpha = atof(argv[2]);
    int beta = atoi(argv[3]);
    int gamma = atoi(argv[4]);
    float alpha_prime = atof(argv[5]);
    int num_repetitions = atoi(argv[6]);

    int* array = genAlphaSortedArray(alpha_prime, n);
    int successes = 0;
    for(rep = 0; rep < num_repetitions; rep++){
        float total_alpha = 0;
        for(i = 0; i < beta; i++){
            int* sub_array = getRandomSubArrayLocal(array, gamma, n);
            total_alpha += calcAlphaSortedness(sub_array, gamma, n);
        }
        float approx_alpha = total_alpha / beta;
        if(approx_alpha >= alpha){
            successes += 1;
        }
    }
    
    printf("Succeeded %d out of %d times (%4f)\n", successes, 
            num_repetitions, (1.0 * successes) / num_repetitions);

    free(array);
    return 0;
}
\end{lstlisting}
\end{document}
